{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week05-01-MPL-starter-code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459kukMTW_NS"
      },
      "source": [
        "# GR5074 - Projects in Advanced Machine Learning\n",
        "# Spring 2022\n",
        "\n",
        "---\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "### Predict World Happiness Rankings (con't) \n",
        "\n",
        "Since you are now familiar with the World Happiness Rankings data, and have played a bit with predictions on traditional ML models, we're ready for the next step with Deep Learning. We're simplifying the AI Model Share code a bit so we can do some in-class illustrations of a fully-connected neural network, and some manipulations to its architecture, including regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. General Setup"
      ],
      "metadata": {
        "id": "L0Hxlv9GR7bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install aimodelshare library\n",
        "! pip install aimodelshare --upgrade"
      ],
      "metadata": {
        "id": "IPJSKbpvR2fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [after class] add all the libraries/classes/functions you imported here\n",
        "# to run Keras\n",
        "\n",
        "import aimodelshare as ai\n",
        "from aimodelshare import download_data \n",
        "from aimodelshare.aws import set_credentials\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n"
      ],
      "metadata": {
        "id": "TYaoomJYc2D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [after class] add all the definitions you are using throughout this notebook\n",
        "\n",
        "apiurl = \"https://c3maq947kb.execute-api.us-east-1.amazonaws.com/prod/m\"\n"
      ],
      "metadata": {
        "id": "2DzW9CM4pmtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get competition data\n",
        "\n",
        "download_data('public.ecr.aws/y2e2a1d6/world_happiness_competition_data-repository:latest')"
      ],
      "metadata": {
        "id": "isp2EYALSTW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "X_train = pd.read_csv('world_happiness_competition_data/X_train.csv')\n",
        "X_test = pd.read_csv('world_happiness_competition_data/X_test.csv')\n",
        "y_train = pd.read_csv('world_happiness_competition_data/y_train.csv')\n",
        "y_train_labels = y_train.idxmax(axis=1)\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "RDVN6mv8SZCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set credentials using modelshare.org username/password\n",
        "\n",
        "set_credentials(apiurl=apiurl)"
      ],
      "metadata": {
        "id": "NSnx-fEkU87B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Competition\n",
        "\n",
        "mycompetition = ai.Competition(apiurl)"
      ],
      "metadata": {
        "id": "mI--3MgaU95S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Pre-process data and save pre-processor function\n"
      ],
      "metadata": {
        "id": "OfhKF0pPTEjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create the preprocessing pipelines for both numeric and categorical data.\n",
        "\n",
        "numeric_features = X_train.drop(\n",
        "    ['Country or region', 'name', 'region', 'sub-region'], \n",
        "    axis=1\n",
        "    )\n",
        "numeric_features = numeric_features.columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_features = ['region', 'sub-region']\n",
        "\n",
        "# Replacing missing values with Modal value and then one hot encoding.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# final preprocessor object set up with ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Fit your preprocessor object\n",
        "preprocess = preprocessor.fit(X_train)"
      ],
      "metadata": {
        "id": "Xapg-3EKS7og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write function to transform data with preprocessor\n",
        "\n",
        "def preprocessor(data):\n",
        "    data.drop(['Country or region', 'name'], axis=1)\n",
        "    preprocessed_data = preprocess.transform(data)\n",
        "    return preprocessed_data"
      ],
      "metadata": {
        "id": "8_tthVjLTYNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check shape of X data after preprocessing it using our new function\n",
        "\n",
        "preprocessor(X_train).shape"
      ],
      "metadata": {
        "id": "Q5YhQT7ETaKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save preprocessor function\n",
        "\n",
        "ai.export_preprocessor(preprocessor,\"\")"
      ],
      "metadata": {
        "id": "EH2gEanmuOFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fit models on preprocessed data and save preprocessor function and model"
      ],
      "metadata": {
        "id": "MLn0KvPwTlPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import Keras functions and classes we will need \n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "#from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "metadata": {
        "id": "_1qAhbrfdLJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some useful definitions in a single location\n",
        "\n",
        "feature_count = preprocessor(X_train).shape[1] # count of features in input data\n",
        "loss = 'categorical_crossentropy'\n",
        "optimizer = 'sgd'\n",
        "epochs = 300\n"
      ],
      "metadata": {
        "id": "sot51_sZdS5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_count"
      ],
      "metadata": {
        "id": "h3RMqbHAYQ_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Let's start with a simple fully-connected neural network (aka MLP)"
      ],
      "metadata": {
        "id": "sayRp6zqbFPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with a very simple fully-connected network (MLP)\n",
        "\n",
        "keras_model = Sequential()\n",
        "keras_model.add(Dense(64, input_dim=feature_count, activation='sigmoid'))\n",
        "keras_model.add(Dense(64, activation='sigmoid'))\n",
        "keras_model.add(Dense(64, activation='sigmoid'))\n",
        "keras_model.add(Dense(5, activation='softmax'))       # why 5 nodes here? \n",
        "                                                      # why softmax?\n",
        "\n",
        "# TRY: other activation functions to try: 'relu', 'tanh' \n",
        "\n",
        "# Compile model\n",
        "keras_model.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=['accuracy']     # add other metrics?\n",
        "    )\n",
        "\n",
        "# Fitting the NN to the Training set\n",
        "keras_model.fit(\n",
        "    preprocessor(X_train), \n",
        "    y_train, \n",
        "    epochs=epochs \n",
        "    )"
      ],
      "metadata": {
        "id": "LVYlSd6VTk2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to ONNX file \n",
        "\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model = model_to_onnx(\n",
        "    keras_model,\n",
        "    framework='keras',\n",
        "    transfer_learning=False,\n",
        "    deep_learning=True        # note this change ;)\n",
        "    )  \n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "JSMiCbHyUpFr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit model: \n",
        "\n",
        "#-- Generate predicted y values\n",
        "# Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index = keras_model.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(\n",
        "    model_filepath = \"model.onnx\",\n",
        "    preprocessor_filepath=\"preprocessor.zip\",\n",
        "    prediction_submission=prediction_labels\n",
        "    )"
      ],
      "metadata": {
        "id": "vE3WcJKNUqfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Does Dropout (regularization) during model training help? \n",
        "\n",
        "Dropout regularization can help ensure that we are not overfitting the model to training data.  \n",
        "\n",
        "Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting."
      ],
      "metadata": {
        "id": "DRbmZmiEeFZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with Dropout regularization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=feature_count, activation='relu'))\n",
        "model.add(Dropout(.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(.5))\n",
        "model.add(Dense(5, activation='softmax')) \n",
        "\n",
        "# try changing proportion of dropout\n",
        "# try selectively applying dropout in some layers\n",
        "                                           \n",
        "# Compile model\n",
        "keras_model.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Fitting the NN to the Training set\n",
        "keras_model.fit(\n",
        "    preprocessor(X_train), \n",
        "    y_train, \n",
        "    epochs=epochs\n",
        "    )\n"
      ],
      "metadata": {
        "id": "V3VDhJDCeyfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to ONNX file \n",
        "\n",
        "onnx_model = model_to_onnx(\n",
        "    keras_model,\n",
        "    framework='keras',\n",
        "    transfer_learning=False,\n",
        "    deep_learning=True        \n",
        "    )  \n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:           \n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "W8VYJ6i9wy_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit model: \n",
        "\n",
        "# Generate predicted y values\n",
        "prediction_column_index = keras_model.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(\n",
        "    model_filepath = \"model.onnx\",\n",
        "    preprocessor_filepath=\"preprocessor.zip\",\n",
        "    prediction_submission=prediction_labels\n",
        "    )"
      ],
      "metadata": {
        "id": "02uXro2px3VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2  Does L1 or L2 regularization during model training help?\n",
        "\n",
        "Recall that L1 or L2 (or both together) add a constraint to the loss function that results in smaller parameters.  Shrinking parameters is a way to ensure that we do not overfit the model to training data.\n",
        "\n",
        "Within keras there are three options: (default is None)\n",
        "* kernel_regularizer(applied to weights),\n",
        "* bias_regularizer(applied to bias unit), and \n",
        "* activity_regularizer(applied to layer activation)."
      ],
      "metadata": {
        "id": "uZjDy4GEgBWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Add L2 regularization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=feature_count, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(5, activation='softmax')) \n",
        "\n",
        "# TRY: turning regularizers off in some layers or keeping them only in one layer \n",
        "                                           \n",
        "# Compile model\n",
        "keras_model.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Fitting the NN to the Training set\n",
        "keras_model.fit(\n",
        "    preprocessor(X_train), \n",
        "    y_train, \n",
        "    epochs=epochs\n",
        "    )\n"
      ],
      "metadata": {
        "id": "XG-TvNu4giy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to ONNX file \n",
        "\n",
        "onnx_model = model_to_onnx(\n",
        "    keras_model,\n",
        "    framework='keras',\n",
        "    transfer_learning=False,\n",
        "    deep_learning=True        \n",
        "    )  \n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:           \n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "IusJjGOazrkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit model: \n",
        "\n",
        "# Generate predicted y values\n",
        "prediction_column_index = keras_model.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(\n",
        "    model_filepath = \"model.onnx\",\n",
        "    preprocessor_filepath=\"preprocessor.zip\",\n",
        "    prediction_submission=prediction_labels\n",
        "    )"
      ],
      "metadata": {
        "id": "vxwerbWkzrVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Lets use Batch Normalization to normalize the weights \n",
        "\n",
        "What happens if we standardize the values of our data with z scores (slightly adjust such that the results rarely equal zero) using Batch Normalization?\n",
        "\n",
        "Batch normalization can be used to standardize the values of weights + bias (the resulting scalar values) before inserting the result into an activation transformation.\n",
        "\n",
        "They can also be used to standardize the output of an activation function AFTER it has been transformed by the activation function (e.g.-sigmoid(sum of weights plus bias)"
      ],
      "metadata": {
        "id": "A2ZvkOFpgnTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=feature_count))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(64))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(5, activation='softmax')) \n",
        "\n",
        "# TRY: turning on Batch normalization for some layer and off for others\n",
        "\n",
        "                                           \n",
        "# Compile model\n",
        "keras_model.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Fitting the NN to the Training set\n",
        "keras_model.fit(\n",
        "    preprocessor(X_train), \n",
        "    y_train, \n",
        "    epochs=epochs \n",
        "    )\n",
        "   "
      ],
      "metadata": {
        "id": "Jp4xjcM0g8vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to ONNX file \n",
        "\n",
        "onnx_model = model_to_onnx(\n",
        "    keras_model,\n",
        "    framework='keras',\n",
        "    transfer_learning=False,\n",
        "    deep_learning=True        \n",
        "    )  \n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:           \n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "y0awJ8z5-D1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit model: \n",
        "\n",
        "# Generate predicted y values\n",
        "prediction_column_index = keras_model.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(\n",
        "    model_filepath = \"model.onnx\",\n",
        "    preprocessor_filepath=\"preprocessor.zip\",\n",
        "    prediction_submission=prediction_labels\n",
        "    )"
      ],
      "metadata": {
        "id": "5wcMPf0b-DiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dADegkTYJ1JY"
      },
      "source": [
        "## 3.4 Experimenting with depth of model (number of hidden layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-0XoC4FJz6i"
      },
      "source": [
        "# Model with L2 regularization\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=feature_count, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(5, activation='softmax')) \n",
        "\n",
        "# TRY: reducing the number of nodes and/or turning regularizers off in some layers \n",
        "\n",
        "\n",
        "# Compile model\n",
        "keras_model.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "\n",
        "# Fitting the NN to the Training set\n",
        "keras_model.fit(\n",
        "    preprocessor(X_train), \n",
        "    y_train, \n",
        "    epochs=epochs, \n",
        "    )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to ONNX file \n",
        "\n",
        "onnx_model = model_to_onnx(\n",
        "    keras_model,\n",
        "    framework='keras',\n",
        "    transfer_learning=False,\n",
        "    deep_learning=True        \n",
        "    )  \n",
        "\n",
        "with open(\"model.onnx\", \"wb\") as f:           \n",
        "    f.write(onnx_model.SerializeToString())"
      ],
      "metadata": {
        "id": "-BgNnfzV-UAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit model: \n",
        "\n",
        "# Generate predicted y values\n",
        "prediction_column_index = keras_model.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(\n",
        "    model_filepath = \"model.onnx\",\n",
        "    preprocessor_filepath=\"preprocessor.zip\",\n",
        "    prediction_submission=prediction_labels\n",
        "    )"
      ],
      "metadata": {
        "id": "abYm78Vy-S-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 experimenting with an increase in epochs\n"
      ],
      "metadata": {
        "id": "Y3P2oPWr-a7j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kykd1cZUOANA"
      },
      "source": [
        "# Model with best L2 regularization run for three times the epochs\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=feature_count, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
        "model.add(Dense(5, activation='softmax')) \n",
        "\n",
        "# TRY: reduce epochs to 20, 50\n",
        "\n",
        "# Compile model\n",
        "keras_model.compile(\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "\n",
        "# Fitting the NN to the Training set\n",
        "keras_model.fit(\n",
        "    preprocessor(X_train), \n",
        "    y_train, \n",
        "    epochs=1000 # note increase here \n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}